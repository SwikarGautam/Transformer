{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s_pk7p_RF8DD"
   },
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m8-rjZwNF8DK"
   },
   "outputs": [],
   "source": [
    "class LinearLayer3d():\n",
    "    \n",
    "    def __init__(self,num_head,d_emd,d_out,biases=False):\n",
    "        self.weights = np.random.randn(num_head,d_emd,d_out)*np.sqrt(2/(num_head*d_emd))# make it xavier\n",
    "        self.biases = np.zeros((1,num_head,1,d_out)) if biases else 0\n",
    "        self.v_w = 0\n",
    "        self.s_w = 0\n",
    "        if biases:\n",
    "            self.v_b = 0\n",
    "            self.s_b = 0\n",
    "        \n",
    "    def forward(self,inps):\n",
    "        self.inputs = inps\n",
    "        return np.matmul(inps,self.weights) + self.biases\n",
    "        \n",
    "    \n",
    "    def backward(self,grads,step_size):\n",
    "        nabla_w = np.matmul(self.inputs.transpose(0,1,-1,-2),grads).sum(axis=0)\n",
    "        nabla_inps = np.matmul(grads,self.weights.transpose(0,-1,1)) \n",
    "        self.weights -= step_size*nabla_w\n",
    "        if self.biases is not 0:\n",
    "            nabla_b = grads.sum(axis=0,keepdims=True).sum(axis=2,keepdims=True)\n",
    "            self.adam_optim(nabla_w, nabla_b,alpha=step_size)\n",
    "        else:\n",
    "            self.adam_optim(nabla_w,alpha=step_size)\n",
    "        return nabla_inps\n",
    "    \n",
    "    def adam_optim(self,d_w,d_b=None,alpha,B1=0.9,B2=0.999):\n",
    "        if self.v_w is 0:\n",
    "            self.v_w = d_w\n",
    "            self.s_w = d_w\n",
    "            \n",
    "        if d_b is not None:\n",
    "            if self.v_b is 0:\n",
    "                self.v_b = d_b\n",
    "                self.s_b = d_b\n",
    "            \n",
    "            self.v_b = B1*self.v_b + (1-B1)*d_b\n",
    "            self.s_b = B2*self.s_b + (1-B2)*(d_b**2)\n",
    "\n",
    "        self.weights -= alpha*self.v_b/(np.sqrt(self.s_b)+1e-8)\n",
    "            \n",
    "        self.v_w = B1*self.v_w + (1-B1)*d_w\n",
    "        self.s_w = B2*self.s_w + (1-B2)*(d_w**2)\n",
    "\n",
    "        self.weights -= alpha*self.v_w/(np.sqrt(self.s_w)+1e-8)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "33pHoq5NF8DN"
   },
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \n",
    "    def forward(self,z,axis=-1):\n",
    "        self.axis = axis\n",
    "        e = np.exp(z-z.max(axis=axis,keepdims=True))\n",
    "        self.output = e/np.nansum(e,axis=axis,keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self,grads):\n",
    "        return self.output*(grads-np.sum(self.output*grads,axis=self.axis,keepdims=True))\n",
    "          \n",
    "    \n",
    "class CrossEntropyLoss():\n",
    "    \n",
    "    def forward(self,out_probs,labels):\n",
    "        self.in_probs = out_probs\n",
    "        self.labels = labels\n",
    "        return np.sum(-labels*np.log(self.in_probs))\n",
    "    \n",
    "    def backward(self):\n",
    "        return -self.labels/self.in_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pLhjFhJTF8DO",
    "outputId": "5f634fbe-9a45-4fa6-b50b-60f3e6f5042a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LayerNorm():\n",
    "    \n",
    "    def forward(self,inps):\n",
    "        self.inps = inps\n",
    "        self.var = np.var(self.inps,axis=-1,keepdims=True)\n",
    "        self.norm = (self.inps-np.mean(inps,axis=-1,keepdims=True))/np.sqrt(self.var)\n",
    "        return self.norm\n",
    "    \n",
    "    def backward(self,grads):\n",
    "        n = self.norm.shape[-1]\n",
    "        return (np.sqrt(self.var)*(n*grads-grads.sum(axis=-1,keepdims=True))-self.norm*((self.inps-\n",
    "                            np.mean(self.inps,axis=-1,keepdims=True))*grads).sum(axis=-1,keepdims=True))/(n*self.var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3I9OsMS8F8DU"
   },
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    \n",
    "    def __init__(self,drop_prob):\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "    def forward(self,inps):\n",
    "        self.dropout = (np.random.rand(*inps.shape) > self.drop_prob)\n",
    "        return np.where(self.dropout,inps,0)\n",
    "        \n",
    "    def backward(self,grads):\n",
    "        return np.where(self.dropout,grads,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention():\n",
    "    \n",
    "    def __init__(self,num_head,d_model):\n",
    "        \n",
    "        self.num_head = num_head\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.q_w1 = LinearLayer3d(num_head,d_model,int(d_model/num_head)) \n",
    "        self.k_w1 = LinearLayer3d(num_head,d_model,int(d_model/num_head))\n",
    "        self.v_w1 = LinearLayer3d(num_head,d_model,int(d_model/num_head))\n",
    "        self.o_w1 = LinearLayer3d(1,d_model,d_model)\n",
    "\n",
    "        self.sm = Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self,query,key,value,valid_lens=None):\n",
    "        self.mh_q = self.q_w1.forward(query) #multiheaded queries\n",
    "        self.mh_k = self.k_w1.forward(key) #multiheaded keys\n",
    "        self.mh_v = self.v_w1.forward(value) #multiheaded values\n",
    "\n",
    "        score = self.mh_q@self.mh_k.transpose(0,1,-1,-2) #attention score\n",
    "        score /= np.sqrt(self.d_model) #scaled attention score\n",
    "        if valid_lens is not None:\n",
    "            mask = MultiHeadAttention.get_mask(valid_lens,score.shape)\n",
    "            score *= mask\n",
    "        self.sm_score = self.sm.forward(score)\n",
    "        a_o = (self.sm_score@self.mh_v) #(10, 8, 9, 64) #attention output\n",
    "\n",
    "        ccat_o = a_o.transpose(0,2,1,3).reshape(-1,1,a_o.shape[2],512) #concatenated output\n",
    "        return self.o_w1.forward(ccat_o) #applying linear transformation\n",
    "\n",
    "\n",
    "    def backward(self,grads,step_size):\n",
    "\n",
    "        d_ccat_o = self.o_w1.backward(grads,step_size)\n",
    "        d_mh_o = d_ccat_o.reshape(-1,d_ccat_o.shape[2],self.num_head,int(self.d_model/self.num_head)).transpose(0,2,1,3)\n",
    "\n",
    "        d_sm_score = d_mh_o@self.mh_v.transpose(0,1,3,2)\n",
    "        d_mh_v = self.sm_score.transpose(0,1,3,2)@d_mh_o\n",
    "        d_score = self.sm.backward(d_sm_score)\n",
    "        d_score *= np.sqrt(self.d_model)\n",
    "\n",
    "        d_mh_q = d_score@self.mh_k\n",
    "        d_mh_k = d_score.transpose(0,1,-1,-2)@self.mh_q\n",
    "\n",
    "        d_q = self.q_w1.backward(d_mh_q,step_size).sum(axis=1,keepdims=True)\n",
    "        d_v = self.v_w1.backward(d_mh_v,step_size).sum(axis=1,keepdims=True)\n",
    "\n",
    "        d_k = self.k_w1.backward(d_mh_k,step_size).sum(axis=1,keepdims=True)\n",
    "        \n",
    "        return d_q,d_k,d_v  #gradients of query,value,key\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_mask(self,valid_lens,shape):\n",
    "        valid_lens = valid_lens.squeeze()\n",
    "        mask = np.ones((shape[0],shape[2]))\n",
    "        x1 = np.arange(1,1+shape[2])[np.newaxis,:].repeat(shape[0],axis=0)\n",
    "        x2 = valid_lens[:,np.newaxis].repeat(shape[2],axis=1)\n",
    "        mask[x1>x2] = -np.inf\n",
    "        return mask[:,:,np.newaxis].repeat(shape[3],axis=2)[:,np.newaxis]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock():\n",
    "    \n",
    "    def __init__(self,num_heads, d_model, drop_prob):\n",
    "        self.Ln1 = LayerNorm()\n",
    "        self.Ln2 = LayerNorm()\n",
    "        self.ff1 = LinearLayer3d(1,d_model,d_model*4,biases=True)\n",
    "        self.ff2 = LinearLayer3d(1,d_model*4,d_model,biases=True)\n",
    "        self.mha = MultiHeadAttention(num_heads, d_model)\n",
    "        self.dp1 = Dropout(drop_prob)\n",
    "        self.dp2 = Dropout(drop_prob)\n",
    "        \n",
    "        \n",
    "    def forward(self,inp):\n",
    "        \n",
    "        attn_o = self.mha.forward(inp,inp,inp)\n",
    "\n",
    "        dp1_o = self.dp1.forward(attn_o,0.85)\n",
    "        norm1 = self.Ln1.forward(dp_o+q)\n",
    "        \n",
    "        ff1_out = self.ff1.forward(norm1)\n",
    "        self.reLu = ff1_out<0\n",
    "        ff1_out[self.reLu] = 0\n",
    "        ff2_out = self.ff2.forward(ff2_in)\n",
    "        \n",
    "        dp2_o = self.dp1.forward(ff2_out)\n",
    "        norm2 = self.Ln2.forward(dp2_o+attn_o)\n",
    "        \n",
    "        return norm2\n",
    "    \n",
    "    \n",
    "    def backward(self,grads,step_size):\n",
    "        \n",
    "        d_add1 = self.Ln2.backward(grads)\n",
    "        d_ff2_out = self.dp2.backward(d_add1)\n",
    "        d_ff1_out = self.ff2.backward(d_ff2_out,step_size)\n",
    "        d_ff1_out[self.reLu] = 0\n",
    "        d_dp_o = self.Ln1.backward(self.ff1.backward(d_ff1_out,step_size))\n",
    "        \n",
    "        d_q1 = d_dp_o\n",
    "        d_attn_o = self.dp1.backward(d_dp_o) + d_add1\n",
    "\n",
    "        d_q2, d_k, d_v = self.mha.backward(d_attn_o, step_size)\n",
    "        \n",
    "        return d_q1 + d_q2 + d_k + d_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock():\n",
    "    def __init__(self,num_heads,d_model,drop_prob):  \n",
    "        \n",
    "        self.Ln1 = LayerNorm()\n",
    "        self.Ln2 = LayerNorm()\n",
    "        self.Ln3 = LayerNorm()\n",
    "        \n",
    "        self.dp1 = Dropout(drop_prob)\n",
    "        self.dp2 = Dropout(drop_prob)\n",
    "        self.dp3 = Dropout(drop_prob)\n",
    "        \n",
    "        self.ff1 = LinearLayer3d(1,d_model,d_model*4)\n",
    "        self.ff2 = LinearLayer3d(1,d_model*4,d_model)\n",
    "        \n",
    "        self.masked_mh_attn = MultiHeadAttention(num_heads,d_model)\n",
    "        self.mh_attn = MultiHeadAttention(num_heads,d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self,inp,enc_o):\n",
    "        \n",
    "        attn1_out = self.masked_mh_attn.forward(inp,inp,inp)\n",
    "        dp1_o = self.dp1.forward(attn1_out)\n",
    "        norm1_out = self.Ln1.forward(dp1_o+inp)\n",
    "        \n",
    "        attn2_out = self.mh_attn.forward(norm1_out,enc_o,enc_o)\n",
    "        dp2_o = self.dp2.forward(attn2_out)\n",
    "        norm2_out = self.Ln2.forward(dp2_o+norm1_out)\n",
    "        \n",
    "        ff1_out = self.ff1.forward(norm2_out)\n",
    "        self.reLu = ff1_out < 0\n",
    "        ff1_out[self.reLu] = 0\n",
    "        ff2_out = self.ff2.forward(ff1_out)\n",
    "        dp3_o = self.dp3.forward(ff2_out)\n",
    "        norm3_out = self.Ln3.forward(dp3_o+norm2_out)\n",
    "        \n",
    "        return norm3_out\n",
    "        \n",
    "        \n",
    "    def backward(self,grads,step_size):\n",
    "        \n",
    "        d_add3 = self.Ln3.backward(grads)\n",
    "        d_ff2 = self.dp3.backward(d_add3)\n",
    "        d_ff1_out = self.ff2.backward(d_ff2,step_size)\n",
    "        d_ff1_out[self.reLu] = 0\n",
    "        d_norm2 = self.ff1.backward(d_ff1_out,step_size) + d_add3\n",
    "        d_add2 = self.Ln2.backward(d_norm2)\n",
    "        \n",
    "        d_mh_attn = self.dp2.backward(d_add2) \n",
    "        d_norm1, d_enc_o1, d_enc_o2 = self.mh_attn.backward(d_mh_attn,step_size)  \n",
    "        d_norm1 += d_add2\n",
    "        d_add1 = self.Ln1.backward(d_norm1)\n",
    "        \n",
    "        d_masked_mh_attn = self.dp1.backward(d_add1)\n",
    "        d_inp, d_k, d_v = self.masked_mh_attn.backward(d_masked_mh_attn,step_size)\n",
    "        \n",
    "        d_inp = d_inp + d_k + d_v + d_add1\n",
    "        d_enc_o = d_enc_o1 + d_enc_o2\n",
    "        return d_inp, d_enc_o\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_embedding(n_pos,d_model):\n",
    "    angles = np.fromfunction(lambda i,j:i/10000**(2*j/d_model),(n_pos,int(d_model/2)))\n",
    "    pos_enc = np.ones((n_pos,d_model))\n",
    "    pos_enc[:,::2] = np.sin(angles)\n",
    "    pos_enc[:,1::2] = np.cos(angles)\n",
    "    return pos_enc\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Rough.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
